{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "image_size=28\n",
    "kernel_size=5\n",
    "num_kernels=32\n",
    "patch_size=4\n",
    "maxpooling_size=[2] \n",
    "maxpooling_stride=[2]\n",
    "maxpooling_pad='same'\n",
    "num_layers=7\n",
    "num_classes=10 \n",
    "embed_dim=64\n",
    "num_heads=4 \n",
    "mlp_dim=64\n",
    "channels=1\n",
    "drop_prob=0.3\n",
    "batch_size=50 \n",
    "epochs=2\n",
    "lr=0.001\n",
    "lr_end=0.00001\n",
    "kl_factor=0.001\n",
    "Training=True \n",
    "continue_training=False\n",
    "saved_model_epochs=800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(image_size=28, kernel_size=5, num_kernels=32, patch_size=4, maxpooling_size=[2], \n",
    "                  maxpooling_stride=[2], maxpooling_pad='same', num_layers=7, num_classes=10, \n",
    "                  embed_dim=64, num_heads=4, mlp_dim=64, channels=1, drop_prob=0.3, batch_size=50, \n",
    "                  epochs=2, lr=0.001, lr_end=0.00001, kl_factor=0.001, Training=True, \n",
    "                  continue_training=False, saved_model_epochs=800):\n",
    "    \n",
    "    PATH = f'./saved_models/VDP_cnn_epoch_{epochs}_kl_{kl_factor}_lr_{lr}/'\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = DensityPropCNN(kernel_size=kernel_size, num_kernel=num_kernels, \n",
    "                                  pooling_size=maxpooling_size, pooling_stride=maxpooling_stride, \n",
    "                                  pooling_pad=maxpooling_pad, units=num_classes)\n",
    "    if continue_training:\n",
    "        saved_model_path = f'./saved_models_new/VDP_cnn_epoch_{saved_model_epochs}_kl_{kl_factor}_lr_latest/'\n",
    "        model.load_state_dict(torch.load(os.path.join(saved_model_path, 'vdp_trans_model.pth')))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # num_train_steps = epochs * int(len(train_dataset) / batch_size)\n",
    "    # def polynomial_decay(step):\n",
    "    #     step = min(step, num_train_steps)\n",
    "    #     return (lr - lr_end) * (1 - step / num_train_steps) ** 0.5 + lr_end\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=polynomial_decay)\n",
    "\n",
    "\n",
    "    # -------------------------  Training loop -------------------------\n",
    "\n",
    "    train_acc = np.zeros(epochs)\n",
    "    valid_acc = np.zeros(epochs)\n",
    "    train_err = np.zeros(epochs)\n",
    "    valid_error = np.zeros(epochs)\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch+1}/{epochs}')\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        ll = []\n",
    "        # --------------------  Training phase  --------------------\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            mu_out, sigma, kl = model(x)\n",
    "            loss_final = nll_gaussian(y, mu_out, torch.clamp(sigma, min=1e-10, max=1e+6)) * 0.001\n",
    "            loss = 0.5 * (loss_final + kl_factor * kl)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(mu_out, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            grads = [param.grad for _, param in model.named_parameters()]\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Step: {batch_idx}, Loss: {loss_final.item()}, kl {kl.item()}, Training accuracy so far: {correct / total:.3f}, sigma norm {np.mean(sigma.detach().numpy())}')\n",
    "            ll.append(loss.item())\n",
    "        train_acc[epoch] = correct / total\n",
    "        train_err[epoch] = total_loss / len(train_loader)\n",
    "        print(f'Training Acc: {train_acc[epoch]}, Training error: {train_err[epoch]}')\n",
    "\n",
    "        # --------------------  Validation phase  --------------------\n",
    "        model.eval()\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(test_loader):\n",
    "            mu_out, sigma, kl = model(x)\n",
    "            total_valid += y.size(0)\n",
    "            loss_v = nll_gaussian(y, mu_out, sigma) * 0.001\n",
    "            val_loss += loss_v.item()\n",
    "            _, predicted = torch.max(mu_out, 1)\n",
    "            correct_valid += (predicted == y).sum().item()\n",
    "\n",
    "        valid_acc[epoch] = correct_valid / total_valid\n",
    "        valid_error[epoch] = val_loss / len(test_loader)\n",
    "        stop = time.time()\n",
    "        os.makedirs(PATH, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(PATH, 'vdp_trans_model.pth'))\n",
    "\n",
    "        print(f'Total Training Time: {stop - start:.2f}s')\n",
    "        print(f'Validation Acc: {valid_acc[epoch]}, Validation error: {valid_error[epoch]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n",
      "Step: 0, Loss: 583.7427368164062, kl 7.222908020019531, Training accuracy so far: 0.200, sigma norm 2.406434873591934e-07\n",
      "Step: 50, Loss: 16.555526733398438, kl 7.166213035583496, Training accuracy so far: 0.683, sigma norm 1.3451494851324242e-07\n",
      "Step: 100, Loss: 33.47002410888672, kl 7.154552459716797, Training accuracy so far: 0.754, sigma norm 1.5209624848466774e-07\n",
      "Step: 150, Loss: 23.23363494873047, kl 7.147308349609375, Training accuracy so far: 0.790, sigma norm 1.1624350548800066e-07\n",
      "Step: 200, Loss: 27.083805084228516, kl 7.13693380355835, Training accuracy so far: 0.806, sigma norm 9.491463259791999e-08\n",
      "Step: 250, Loss: 38.787254333496094, kl 7.12849235534668, Training accuracy so far: 0.815, sigma norm 1.0768489744350518e-07\n",
      "Step: 300, Loss: 23.597301483154297, kl 7.114316940307617, Training accuracy so far: 0.819, sigma norm 9.5577327385854e-08\n",
      "Step: 350, Loss: 35.03846740722656, kl 7.102791786193848, Training accuracy so far: 0.826, sigma norm 1.1729179760777697e-07\n",
      "Step: 400, Loss: 30.356298446655273, kl 7.097225189208984, Training accuracy so far: 0.835, sigma norm 1.2195415877158666e-07\n",
      "Step: 450, Loss: 37.814414978027344, kl 7.089672088623047, Training accuracy so far: 0.841, sigma norm 1.908016713514371e-07\n",
      "Step: 500, Loss: 8.646754264831543, kl 7.081183433532715, Training accuracy so far: 0.845, sigma norm 7.427148318583932e-08\n",
      "Step: 550, Loss: 25.009233474731445, kl 7.073451042175293, Training accuracy so far: 0.850, sigma norm 7.126405421331583e-08\n",
      "Step: 600, Loss: 14.150607109069824, kl 7.064693450927734, Training accuracy so far: 0.853, sigma norm 7.271571433875579e-08\n",
      "Step: 650, Loss: 14.481389999389648, kl 7.05289363861084, Training accuracy so far: 0.854, sigma norm 1.0795106675232091e-07\n",
      "Step: 700, Loss: 1.5725489854812622, kl 7.04485559463501, Training accuracy so far: 0.858, sigma norm 6.030518306943122e-08\n",
      "Step: 750, Loss: 34.45438766479492, kl 7.038590431213379, Training accuracy so far: 0.860, sigma norm 8.482894742201097e-08\n",
      "Step: 800, Loss: 15.378138542175293, kl 7.024349689483643, Training accuracy so far: 0.862, sigma norm 1.0984069120922868e-07\n",
      "Step: 850, Loss: 76.90708923339844, kl 7.0159687995910645, Training accuracy so far: 0.863, sigma norm 7.791282996549853e-08\n",
      "Step: 900, Loss: 36.871063232421875, kl 7.007615089416504, Training accuracy so far: 0.865, sigma norm 8.381035598858944e-08\n",
      "Step: 950, Loss: 3.281804323196411, kl 6.999066352844238, Training accuracy so far: 0.867, sigma norm 1.0071248368603847e-07\n",
      "Step: 1000, Loss: 9.450039863586426, kl 6.991605758666992, Training accuracy so far: 0.869, sigma norm 1.0114916904058191e-07\n",
      "Step: 1050, Loss: 8.627108573913574, kl 6.9813079833984375, Training accuracy so far: 0.871, sigma norm 5.313332351875033e-08\n",
      "Step: 1100, Loss: 7.707398891448975, kl 6.973733425140381, Training accuracy so far: 0.872, sigma norm 4.518929941355054e-08\n",
      "Step: 1150, Loss: 12.022103309631348, kl 6.970852851867676, Training accuracy so far: 0.875, sigma norm 8.650498273254925e-08\n",
      "Training Acc: 0.8783, Training error: 11.540573796309376\n",
      "Total Training Time: 16.59s\n",
      "Validation Acc: 0.9073, Validation error: 12.071703320541419\n",
      "Epoch: 2/2\n",
      "Step: 0, Loss: 7.4801812171936035, kl 6.968297004699707, Training accuracy so far: 0.940, sigma norm 7.18108381647653e-08\n",
      "Step: 50, Loss: 0.5885562896728516, kl 6.961733818054199, Training accuracy so far: 0.920, sigma norm 3.6624719257360994e-08\n",
      "Step: 100, Loss: 5.776325702667236, kl 6.9605393409729, Training accuracy so far: 0.929, sigma norm 5.756372445375746e-08\n",
      "Step: 150, Loss: 9.50652027130127, kl 6.958803176879883, Training accuracy so far: 0.930, sigma norm 5.036091010879318e-08\n",
      "Step: 200, Loss: 7.76902437210083, kl 6.9533843994140625, Training accuracy so far: 0.929, sigma norm 6.973698418732965e-08\n",
      "Step: 250, Loss: 10.043858528137207, kl 6.949331283569336, Training accuracy so far: 0.930, sigma norm 6.964690868471735e-08\n",
      "Step: 300, Loss: 2.0555779933929443, kl 6.941300868988037, Training accuracy so far: 0.927, sigma norm 4.5950660165772206e-08\n",
      "Step: 350, Loss: 6.412169933319092, kl 6.937103271484375, Training accuracy so far: 0.928, sigma norm 7.700653270603652e-08\n",
      "Step: 400, Loss: 13.423420906066895, kl 6.937536239624023, Training accuracy so far: 0.931, sigma norm 8.43059098087906e-08\n",
      "Step: 450, Loss: 20.033565521240234, kl 6.937254905700684, Training accuracy so far: 0.933, sigma norm 1.072250910283401e-07\n",
      "Step: 500, Loss: 0.48180216550827026, kl 6.93671989440918, Training accuracy so far: 0.934, sigma norm 2.713066926673946e-08\n",
      "Step: 550, Loss: 7.389613628387451, kl 6.935796737670898, Training accuracy so far: 0.935, sigma norm 4.107596751623532e-08\n",
      "Step: 600, Loss: 3.875828504562378, kl 6.936209678649902, Training accuracy so far: 0.936, sigma norm 5.292675808732383e-08\n",
      "Step: 650, Loss: 1.15552818775177, kl 6.9358625411987305, Training accuracy so far: 0.937, sigma norm 2.3423858408477827e-08\n",
      "Step: 700, Loss: 0.2848040461540222, kl 6.935919761657715, Training accuracy so far: 0.938, sigma norm 3.079412991269237e-08\n",
      "Step: 750, Loss: 4.623286724090576, kl 6.9364190101623535, Training accuracy so far: 0.938, sigma norm 4.6860510138913014e-08\n",
      "Step: 800, Loss: 2.4967164993286133, kl 6.935879230499268, Training accuracy so far: 0.939, sigma norm 4.449946189311049e-08\n",
      "Step: 850, Loss: 16.363061904907227, kl 6.9371867179870605, Training accuracy so far: 0.939, sigma norm 5.063768071522645e-08\n",
      "Step: 900, Loss: 7.30229377746582, kl 6.938143730163574, Training accuracy so far: 0.940, sigma norm 6.021019061108746e-08\n",
      "Step: 950, Loss: 0.4170098304748535, kl 6.938583850860596, Training accuracy so far: 0.940, sigma norm 3.350595179085758e-08\n",
      "Step: 1000, Loss: 0.8283188939094543, kl 6.939135551452637, Training accuracy so far: 0.941, sigma norm 3.306569951178062e-08\n",
      "Step: 1050, Loss: 0.360063761472702, kl 6.939485549926758, Training accuracy so far: 0.942, sigma norm 2.1827622376235922e-08\n",
      "Step: 1100, Loss: 1.0164499282836914, kl 6.941078186035156, Training accuracy so far: 0.942, sigma norm 2.576277857713194e-08\n",
      "Step: 1150, Loss: 2.0971662998199463, kl 6.943612575531006, Training accuracy so far: 0.943, sigma norm 3.5969325296036914e-08\n",
      "Training Acc: 0.9447166666666666, Training error: 2.4677715708790733\n",
      "Total Training Time: 31.20s\n",
      "Validation Acc: 0.9561, Validation error: 3.1186825647321528\n"
     ]
    }
   ],
   "source": [
    "main_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
